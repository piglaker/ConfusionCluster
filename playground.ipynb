{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ChineseBertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1266,  776, 3221,  103, 1744, 4638, 7674, 6963,  511,  102]]), 'pinyin_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  7, 10, 14,  3,  0,  0,  0,  0, 15, 14,\n",
      "         19, 12,  1,  0,  0,  0, 24, 13, 14,  4,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0, 12, 26, 20,  2,  0,  0,  0,  0,  9, 10,  5,  0,  0,  0,\n",
      "          0,  0, 24, 13, 20, 26,  3,  0,  0,  0,  9, 26,  1,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['中|0.711', '我|0.2488', '祖|0.016', '法|0.0057', '美|0.0048', '全|0.0042', '韩|0.0015', '英|0.0011', '两|0.0008', '王|0.0006']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from chinesebert import ChineseBertForMaskedLM, ChineseBertTokenizerFast, ChineseBertConfig\n",
    "\n",
    "pretrained_model_name = \"junnyu/ChineseBERT-base\"\n",
    "\n",
    "tokenizer = ChineseBertTokenizerFast.from_pretrained(pretrained_model_name)\n",
    "chinese_bert = ChineseBertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "text = \"北京是[MASK]国的首都。\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "maskpos = 4\n",
    "\n",
    "with torch.no_grad():\n",
    "    o = chinese_bert(**inputs)\n",
    "    value, index = o.logits.softmax(-1)[0, maskpos].topk(10)\n",
    "\n",
    "pred_tokens = tokenizer.convert_ids_to_tokens(index.tolist())\n",
    "pred_values = value.tolist()\n",
    "\n",
    "outputs = []\n",
    "for t, p in zip(pred_tokens, pred_values):\n",
    "    outputs.append(f\"{t}|{round(p,4)}\")\n",
    "print(outputs)\n",
    "\n",
    "# base  ['中|0.711', '我|0.2488', '祖|0.016', '法|0.0057', '美|0.0048', '全|0.0042', '韩|0.0015', '英|0.0011', '两|0.0008', '王|0.0006']\n",
    "# large ['中|0.8341', '我|0.1479', '祖|0.0157', '全|0.0007', '国|0.0005', '帝|0.0001', '该|0.0001', '法|0.0001', '一|0.0001', '咱|0.0001']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ChineseBertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2472, 3821, 823, 2548, 4638, 2650, 1196, 1137, 3227, 749, 1762, 5401, 1744, 1469, 686, 4518, 5745, 1741, 102], 'pinyin_ids': [0, 0, 0, 0, 0, 0, 0, 0, 11, 26, 2, 0, 0, 0, 0, 0, 17, 26, 20, 4, 0, 0, 0, 0, 30, 14, 1, 0, 0, 0, 0, 0, 9, 10, 2, 0, 0, 0, 0, 0, 9, 10, 5, 0, 0, 0, 0, 0, 7, 10, 14, 1, 0, 0, 0, 0, 15, 26, 4, 0, 0, 0, 0, 0, 25, 26, 1, 0, 0, 0, 0, 0, 29, 14, 6, 19, 3, 0, 0, 0, 17, 10, 5, 0, 0, 0, 0, 0, 31, 6, 14, 4, 0, 0, 0, 0, 18, 10, 14, 3, 0, 0, 0, 0, 12, 26, 20, 2, 0, 0, 0, 0, 13, 10, 2, 0, 0, 0, 0, 0, 24, 13, 14, 4, 0, 0, 0, 0, 15, 14, 10, 4, 0, 0, 0, 0, 11, 6, 19, 4, 0, 0, 0, 0, 28, 10, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59483/3963314147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pinyin_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pinyin_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from chinesebert import ChineseBertTokenizerFast\n",
    "\n",
    "tokenizer = ChineseBertTokenizerFast.from_pretrained(\"junnyu/ChineseBERT-base\")\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "textlist = [\"弗洛伊德的悲剧凸显了在美国和世界范围\", \"紧迫性和重要性，国际社会必须立\", \"那些存在严重种族主义、种族歧视\", \"中方对巴基斯坦开普省发\"]\n",
    "batch_list = [tokenizer(t) for t in textlist]\n",
    "for i in textlist:\n",
    "  print(tokenizer(i))\n",
    "  break\n",
    "batch = collate_fn(batch_list)\n",
    "#print(batch.to(\"cuda:0\"))\n",
    "\n",
    "for i, e in enumerate(batch):\n",
    "\n",
    "  print(e[\"pinyin_ids\"] == tokenizer(textlist[i])[\"pinyin_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[ 0,  0,  0,  0,  0,  0,  0,  0, 11, 26,  2,  0,  0,  0,  0,  0, 17, 26,\n",
    "         20,  4,  0,  0,  0,  0, 30, 14,  1,  0,  0,  0,  0,  0,  9, 10,  2,  0,\n",
    "          0,  0,  0,  0,  9, 10,  5,  0,  0,  0,  0,  0,  7, 10, 14,  1,  0,  0,\n",
    "          0,  0, 15, 26,  4,  0,  0,  0,  0,  0, 25, 26,  1,  0,  0,  0,  0,  0,\n",
    "         29, 14,  6, 19,  3,  0,  0,  0, 17, 10,  5,  0,  0,  0,  0,  0, 31,  6,\n",
    "         14,  4,  0,  0,  0,  0, 18, 10, 14,  3,  0,  0,  0,  0, 12, 26, 20,  2,\n",
    "          0,  0,  0,  0, 13, 10,  2,  0,  0,  0,  0,  0, 24, 13, 14,  4,  0,  0,\n",
    "          0,  0, 15, 14, 10,  4,  0,  0,  0,  0, 11,  6, 19,  4,  0,  0,  0,  0,\n",
    "         28, 10, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cos(torch.tensor([0.0]), torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_model_name_path=\"hfl/chinese-roberta-wwm-ext\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tmp.pkl\", \"rb\") as f:\n",
    "    score = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 21128])\n"
     ]
    }
   ],
   "source": [
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk3 = score.topk(3, dim=1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2769,  511,  872],\n",
       "        [ 872, 2644, 2769],\n",
       "        [1962,  812, 2769],\n",
       "        [ 106,  131,  117],\n",
       "        [2769, 6443, 2218],\n",
       "        [3221, 1373, 4263],\n",
       "        [2476, 2769, 2484],\n",
       "        [4263,  136, 2695],\n",
       "        [3152, 2769,  872],\n",
       "        [ 511, 8013,  117],\n",
       "        [2769,  511,  872],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511,  106,  136],\n",
       "        [ 511,  106, 8013],\n",
       "        [ 511,  106, 8013],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511,  106, 8013],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 106, 2769,  511],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [2769,  872, 6468],\n",
       "        [ 511, 8013,  106],\n",
       "        [2769,  872, 6468],\n",
       "        [ 872, 2769, 6468],\n",
       "        [2769,  872, 6468],\n",
       "        [ 511, 8013,  106],\n",
       "        [2769,  872, 6468],\n",
       "        [ 136,  511,  106],\n",
       "        [ 511,  106, 8013],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [2769,  511,  106],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511,  106, 8013],\n",
       "        [ 511,  106, 8013],\n",
       "        [2769,  872, 6468],\n",
       "        [ 511, 8013,  106],\n",
       "        [ 511,  106, 8013],\n",
       "        [2769,  872,  106],\n",
       "        [ 106,  511, 8013],\n",
       "        [ 511, 8013,  106],\n",
       "        [2769,  872, 2476]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecce3fd1629bcba61391e0f572df4e0d9ae2f07d982e7e7455d71f206dfcfd55"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('3090': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
